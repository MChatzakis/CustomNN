{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGZawn6zjw8B"
      },
      "source": [
        "# Homework 1 - Emmanouil Chatzakis, 353068\n",
        "\n",
        "In this homework, we're going to build a neural network without using deep learning packages.\n",
        "\n",
        "Specifically, we're going to build a neural network that annotate cells to cell types in the Pancreas tissue. To do so, we'll need a few building blocks:\n",
        "- Fully-connected layer, $f(X)=X \\cdot W + \\vec{b}$\n",
        "- Nonlinearity layer (ReLU in this homework)\n",
        "- Loss function (Cross-entropy in this homework)\n",
        "- Backprop algorithm - a stochastic gradient descent with backpropageted gradients\n",
        "\n",
        "Reference: \n",
        "- https://github.com/yandexdataschool/Practical_DL/tree/fall23\n",
        "- https://github.com/theislab/scarches-reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "U_M_6S9Ujw8C",
        "outputId": "173ca758-099b-4fc0-9056-dd5717d38034"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF1dyLBTjw8D"
      },
      "source": [
        "## 0. Create the Parent Class 'Layer' (0 pt)\n",
        "\n",
        "As we learned in the lecture, we need layers that can do both forward pass and backward pass. Here we define the parent class of all layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "id": "fQv1zgRUjw8D"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "\n",
        "    \"\"\"\n",
        "    Each layer performs two things:\n",
        "    1. Forward pass: Process input to get output: output = layer.forward(input)\n",
        "    2. Backward pass: Back-propagate gradients through itself: grad_input = layer.backward(input, grad_output)\n",
        "\n",
        "    The layers that contain learnable parameters also update their parameters during layer.backward.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize layer parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        # Here we use a dummy layer that does nothing.\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Takes input data of shape [batch, input_dims], returns output data [batch, output_dims]\n",
        "        \"\"\"\n",
        "\n",
        "        # Here we use a dummy layer that returns the input.\n",
        "        return input\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Performs a backpropagation step through the layer.\n",
        "\n",
        "        We need to apply the chain rule to compute the gradients of the input x:\n",
        "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
        "\n",
        "        Grad_output provides us d loss / d layer, so we only need to multiply it by d layer / d x.\n",
        "\n",
        "        Note that if the layer has trainable parameters, we also need to update them using d loss / d layer.\n",
        "        \"\"\"\n",
        "\n",
        "        # The gradient of a dummy layer is grad_output\n",
        "        return grad_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Q_CChKjw8E"
      },
      "source": [
        "## 1. Build layers\n",
        "\n",
        "We will shortly introduce our dataset's structure in this section, and it should be enough to complete the functions in this section. However, if you find knowing the dataset structure is super helpful to implement the functions of this section, *you can jump to Section 2* and check the dataset structure before going through this section.\n",
        "\n",
        "### 1.1. Nonlinearity layer - ReLU (1 pt) \n",
        "\n",
        "We start from the simplest layer: nonlinearity layer. It simply applies a nonlinearity to each element of your network and it contains no trainable parameter.\n",
        "\n",
        "Here we implement ReLU. \n",
        "\n",
        "Check this link to review ReLU if needed: https://www.v7labs.com/blog/neural-networks-activation-functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "Sj5NNWB9jw8E"
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        ReLU layer applies elementwise rectified linear to the elements in the inputs.\n",
        "        There is nothing to initialize in this simple implementation.\n",
        "        \"\"\"\n",
        "\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Apply elementwise ReLU to the input\n",
        "        \"\"\"\n",
        "\n",
        "        # ReLU keeps only the positive elements and discards the negative elements\n",
        "        output = np.maximum(0, input)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Compute the gradient of loss.\n",
        "        \"\"\"\n",
        "\n",
        "        d_input = (\n",
        "            input > 0\n",
        "        )  # 1 if input > 0, 0 otherwise (gradient of f'(x) = 1 for x > 0, 0 otherwise)\n",
        "        grad_input = d_input * grad_output\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aihMC0K8jw8E"
      },
      "source": [
        "### 1.2. Fully-connected Layer (2 pts)\n",
        "\n",
        "After implementing the simplest layer, we come to a more complicated one: a fully-connected layer. Unlike a nonlinearity layer, a fully-connected layer has trainable parameters.\n",
        "\n",
        "A fully-connected layer applies an affine transformation. It can be described as: \n",
        "$$f(X)= X \\cdot W + \\vec b ,$$\n",
        "\n",
        "where\n",
        "* X is the input of shape [batch_size, input_dims],\n",
        "* W is a weight matrix [input_dims, output_dims],\n",
        "* and b is a vector of outputs_dims biases.\n",
        "\n",
        "W and b are\n",
        "* initialized when the layer is created,\n",
        "* and updated each time backward is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "id": "VOkSrmYzjw8E"
      },
      "outputs": [],
      "source": [
        "class Fully_connected(Layer):\n",
        "    def __init__(self, input_dims, output_dims, learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        Fully_connected layer: f(x) = <x*W> + b\n",
        "        We initialize W, b, and learning rate to update W and b.\n",
        "        \"\"\"\n",
        "        # Here we use normal initialization for W and zero initialization for b\n",
        "\n",
        "        # Using normal distribution to initialize weights\n",
        "        self.scale_factor = 0.01\n",
        "        self.weights = np.random.normal(\n",
        "            scale=self.scale_factor, size=(input_dims, output_dims)\n",
        "        )\n",
        "        # Use this for random init:\n",
        "        # self.weights = np.random.randn(input_dims, output_dims) * self.scale_factor\n",
        "\n",
        "        self.biases = np.zeros((1, output_dims))\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Perform f(x) = <x*W> + b .\n",
        "\n",
        "        input shape: [batch, input_dims]\n",
        "        output shape: [batch, output_dims]\n",
        "        \"\"\"\n",
        "\n",
        "        output = np.dot(input, self.weights) + self.biases\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        # Calculate  d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
        "\n",
        "        # Calculate the grad_input to back propagate:\n",
        "        grad_input = np.dot(grad_output, self.weights.T)\n",
        "\n",
        "        # Here we perform a stochastic gradient descent (SGD) step: x = x - learning_rate * gradient_of_x.\n",
        "        # Update parameter W\n",
        "        self.weights = self.weights - self.learning_rate * np.dot(input.T, grad_output)\n",
        "\n",
        "        # Update parameter b\n",
        "        self.biases = self.biases - self.learning_rate * np.sum(\n",
        "            grad_output, axis=0, keepdims=True\n",
        "        )\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X Shape: (10, 3)\n",
            "\n",
            "W Shape: (3, 4)\n",
            "b Shape: (1, 4)\n",
            "\n",
            "Output Shape: (10, 4)\n",
            "RELU Output Shape: (10, 4)\n",
            "\n",
            "Output:\n",
            " [[-0.00244976  0.00433787 -0.02100049 -0.04402913]\n",
            " [ 0.00076     0.01434846  0.01332934 -0.00805487]\n",
            " [ 0.00239852  0.00366827 -0.00921458 -0.01917348]\n",
            " [-0.00549772 -0.00261837 -0.01712191 -0.02705211]\n",
            " [-0.00096332  0.00172639 -0.00991357 -0.01991455]\n",
            " [-0.00388643  0.00421801  0.0175704   0.01426838]\n",
            " [ 0.01334501  0.00756629  0.01348222  0.0197822 ]\n",
            " [ 0.00087989  0.00606686  0.01261434  0.00798583]\n",
            " [ 0.00839961  0.00617115 -0.03852329 -0.063096  ]\n",
            " [-0.01131398 -0.00275329  0.00511243  0.0012724 ]]\n",
            "\n",
            "RELU Output:\n",
            " [[0.         0.00433787 0.         0.        ]\n",
            " [0.00076    0.01434846 0.01332934 0.        ]\n",
            " [0.00239852 0.00366827 0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.00172639 0.         0.        ]\n",
            " [0.         0.00421801 0.0175704  0.01426838]\n",
            " [0.01334501 0.00756629 0.01348222 0.0197822 ]\n",
            " [0.00087989 0.00606686 0.01261434 0.00798583]\n",
            " [0.00839961 0.00617115 0.         0.        ]\n",
            " [0.         0.         0.00511243 0.0012724 ]]\n"
          ]
        }
      ],
      "source": [
        "# Very basic testing to ensure that syntax is correct.\n",
        "batch_size = 10\n",
        "\n",
        "X = np.random.randn(batch_size, 3)\n",
        "print(\"X Shape:\", X.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "layer = Fully_connected(3, 4)\n",
        "print(\"W Shape:\", layer.weights.shape)\n",
        "print(\"b Shape:\", layer.biases.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "output = layer.forward(X)\n",
        "print(\"Output Shape:\", output.shape)\n",
        "RELU_output = ReLU().forward(output)\n",
        "print(\"RELU Output Shape:\", RELU_output.shape)\n",
        "\n",
        "print()\n",
        "print(\"Output:\\n\", output)\n",
        "print(\"\\nRELU Output:\\n\", RELU_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvXCRmwKjw8G"
      },
      "source": [
        "### 1.3. The loss function - softmax cross-entropy (2 pts)\n",
        "\n",
        "Our model outputs the logits. Since we want to predict probabilities, it would be logical for us to apply softmax nonlinearity to our output logits and compute loss with predicted probabilities.\n",
        "\n",
        "If you are not familiar with softmax or cross-entropy loss, you can check:\n",
        "- Softmax: https://en.wikipedia.org/wiki/Softmax_function\n",
        "- Cross-entropy loss: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy\n",
        "\n",
        "\n",
        "(We encourage you to first write down the expression for cross-entropy as a function of softmax logits. You can then try to rewrite it into a more concise form.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch_softmax(logits):\n",
        "    \"\"\"\n",
        "    Compute softmax activations.\n",
        "    logits shape: [batch_size, n_classes]\n",
        "    output shape: [batch_size, n_classes]\n",
        "    \"\"\"\n",
        "\n",
        "    expon_logits = np.exp(logits)\n",
        "    softmax_logists = expon_logits / np.sum(expon_logits, axis=1, keepdims=True)\n",
        "\n",
        "    return softmax_logists\n",
        "\n",
        "\n",
        "def toOneHot(true_index, num_classes):\n",
        "    \"\"\"\n",
        "    Convert class index to one hot vector.\n",
        "    true_index shape: [batch_size, 1]\n",
        "    \"\"\"\n",
        "\n",
        "    one_hot_batch = np.zeros((true_index.shape[0], num_classes))\n",
        "    one_hot_batch[np.arange(true_index.shape[0]), true_index] = 1\n",
        "\n",
        "    return one_hot_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Synthetic Logits Shape: (5, 4)\n",
            "Softmax Logits Shape: (5, 4)\n",
            "\n",
            "Logits:\n",
            " [[-1.70627019  1.9507754  -0.50965218 -0.4380743 ]\n",
            " [-1.25279536  0.77749036 -1.61389785 -0.21274028]\n",
            " [-0.89546656  0.3869025  -0.51080514 -1.18063218]\n",
            " [-0.02818223  0.42833187  0.06651722  0.3024719 ]\n",
            " [-0.63432209 -0.36274117 -0.67246045 -0.35955316]]\n",
            "\n",
            "Softmax Logits:\n",
            " [[0.0214546  0.83129512 0.0709913  0.07625898]\n",
            " [0.08235507 0.62723784 0.05739388 0.23301321]\n",
            " [0.14649488 0.52813903 0.2152181  0.110148  ]\n",
            " [0.19724767 0.31136761 0.21683997 0.27454475]\n",
            " [0.2178249  0.28579434 0.20967384 0.28670691]]\n"
          ]
        }
      ],
      "source": [
        "batch_size = 5\n",
        "classes = 4\n",
        "\n",
        "synthetic_logits = np.random.randn(batch_size, classes)\n",
        "print(\"Synthetic Logits Shape:\", synthetic_logits.shape)\n",
        "\n",
        "softmax_logits = batch_softmax(synthetic_logits)\n",
        "print(\"Softmax Logits Shape:\", softmax_logits.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Logits:\\n\", synthetic_logits)\n",
        "print()\n",
        "print(\"Softmax Logits:\\n\", softmax_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Labels Shape: (4,)\n",
            "One Hot Labels:\n",
            " [[1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]]\n",
            "One Hot Labels Shape: (4, 5)\n"
          ]
        }
      ],
      "source": [
        "true_labels = np.array([0, 0, 2, 3])\n",
        "classes = 5\n",
        "print(\"True Labels Shape:\", true_labels.shape)\n",
        "\n",
        "one_hot_true_labels = toOneHot(true_labels, classes)\n",
        "print(\"One Hot Labels:\\n\", one_hot_true_labels)\n",
        "print(\"One Hot Labels Shape:\", one_hot_true_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "id": "jPw8k28mjw8G"
      },
      "outputs": [],
      "source": [
        "def softmax_crossentropy_with_logits(logits, reference_answers):\n",
        "    \"\"\"\n",
        "    Compute cross-entropy loss for each sample from output logits [batch,n_classes] and reference_answers [batch].\n",
        "    Note that the reference_answers are not one-hot labels. Instead, they are the index of the categories, e.g., 2 instead of [0,0,1].\n",
        "    The output (xentropy) shape should be [batch,1] or [batch].\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the cross-entropy loss based on output logits and reference_answers\n",
        "    # Logits is the raw output without softmax. So, it is better to first derive and get the simplest form of the softmax cross-entropy before implementation.\n",
        "\n",
        "    softmax_logits = batch_softmax(logits)\n",
        "    reference_answers_one_hot = toOneHot(reference_answers, logits.shape[1])\n",
        "\n",
        "    # Element-wise multiplication\n",
        "    xentropy = -np.sum(\n",
        "        reference_answers_one_hot * np.log(softmax_logits), axis=1, keepdims=True\n",
        "    )\n",
        "\n",
        "    return xentropy\n",
        "\n",
        "\n",
        "def grad_softmax_crossentropy_with_logits(logits, reference_answers):\n",
        "    \"\"\"\n",
        "    Compute cross-entropy gradient from output logits [batch,n_classes] and reference_answers [batch].\n",
        "    Note that the reference_answers are not one-hot labels. Instead, they are the index of the categories, e.g., 2 instead of [0,0,1].\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the cross-entropy gradient based on output logits and reference_answers\n",
        "    # Hint: Same as before, it is better to first derive and get the simplest form of the softmax cross-entropy gradient before implementation.\n",
        "    batch_size, n_classes = logits.shape\n",
        "    y_one_hot = toOneHot(reference_answers, n_classes)\n",
        "\n",
        "    softmax_logits = batch_softmax(logits)\n",
        "    grad_input = (-y_one_hot + softmax_logits) / batch_size\n",
        "\n",
        "    return grad_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape: (5, 4)\n",
            "Reference Answers (oneHot):\n",
            " [[1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]]\n",
            "\n",
            "\n",
            "--- Testing cross-entropy loss ---\n",
            "Loss shape: (5, 1)\n",
            "\n",
            "Logits:\n",
            " [[-0.81314628 -1.7262826   0.17742614 -0.40178094]\n",
            " [-1.63019835  0.46278226 -0.90729836  0.0519454 ]\n",
            " [ 0.72909056  0.12898291  1.13940068 -1.23482582]\n",
            " [ 0.40234164 -0.68481009 -0.87079715 -0.57884966]\n",
            " [-0.31155253  0.05616534 -1.16514984  0.90082649]]\n",
            "\n",
            "log(softmax):\n",
            " [[-1.72328713 -2.63642345 -0.7327147  -1.31192178]\n",
            " [-2.80617577 -0.71319517 -2.08327579 -1.12403203]\n",
            " [-1.16200814 -1.7621158  -0.75169802 -3.12592453]\n",
            " [-0.68913478 -1.77628651 -1.96227357 -1.67032609]\n",
            " [-1.82966309 -1.46194522 -2.6832604  -0.61728407]]\n",
            "\n",
            "Loss (per sample):\n",
            " [[1.72328713]\n",
            " [2.80617577]\n",
            " [0.75169802]\n",
            " [1.67032609]\n",
            " [1.46194522]]\n",
            "\n",
            "--- Testing cross-entropy loss gradient ---\n",
            "Gradient shape: (5, 4)\n",
            "Gradient:\n",
            " [[-0.1643043   0.01432339  0.09612051  0.0538604 ]\n",
            " [-0.18791287  0.09801516  0.02490433  0.06499337]\n",
            " [ 0.06257146  0.03433625 -0.10568697  0.00877927]\n",
            " [ 0.10040205  0.03385311  0.02810771 -0.16236286]\n",
            " [ 0.03209352 -0.15364301  0.01366799  0.10788149]]\n"
          ]
        }
      ],
      "source": [
        "# Do some testing here to ensure that the syntax is correct.\n",
        "batch_size = 5\n",
        "classes = 4\n",
        "\n",
        "synthetic_logits = np.random.randn(batch_size, classes)\n",
        "print(\"Logits shape:\", synthetic_logits.shape)\n",
        "\n",
        "reference_answers = np.array([0, 0, 2, 3, 1])\n",
        "print(\"Reference Answers (oneHot):\\n\", toOneHot(reference_answers, classes))\n",
        "print()\n",
        "print(\"\\n--- Testing cross-entropy loss ---\")\n",
        "\n",
        "loss = softmax_crossentropy_with_logits(synthetic_logits, reference_answers)\n",
        "print(\"Loss shape:\", loss.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Logits:\\n\", synthetic_logits)\n",
        "print()\n",
        "print(\"log(softmax):\\n\", np.log(batch_softmax(synthetic_logits)))\n",
        "print()\n",
        "print(\"Loss (per sample):\\n\", loss)\n",
        "\n",
        "print(\"\\n--- Testing cross-entropy loss gradient ---\")\n",
        "\n",
        "# Test also the grads!\n",
        "gradient = grad_softmax_crossentropy_with_logits(synthetic_logits, reference_answers)\n",
        "print(\"Gradient shape:\", gradient.shape)\n",
        "print(\"Gradient:\\n\", gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4. Network and Forward pass (1 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XErb4TrFjw8G"
      },
      "source": [
        "First, we define network as a list of layers, each applied on top of previous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": true,
        "id": "TOVs-C5pjw8G"
      },
      "outputs": [],
      "source": [
        "network = []\n",
        "network.append(Fully_connected(1000, 256))\n",
        "network.append(ReLU())\n",
        "network.append(Fully_connected(256, 64))\n",
        "network.append(ReLU())\n",
        "network.append(Fully_connected(64, 8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we implement the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "collapsed": true,
        "id": "qUD5bRhBjw8G"
      },
      "outputs": [],
      "source": [
        "def forward(network, X):\n",
        "    \"\"\"\n",
        "    Compute the output of all network layers by applying them sequentially.\n",
        "    Note that we should return a list of outputs for each layer since we need them for backwrad pass.\n",
        "    \"\"\"\n",
        "\n",
        "    outputs = []\n",
        "    input = X\n",
        "\n",
        "    # Using network to get a list of outputs for each layer\n",
        "    for layer in network:\n",
        "        input = layer.forward(input)\n",
        "        outputs.append(input)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def predict(network, X):\n",
        "    \"\"\"\n",
        "    Use network to predict the result for each sample. Since we are doing classification, the result should be the index of the most likely class.\n",
        "    \"\"\"\n",
        "\n",
        "    # ToDo: using network to get the final output of the model\n",
        "    outputs = forward(network, X)\n",
        "    final_layer_output = outputs[-1]\n",
        "\n",
        "    prediction_probabilities = batch_softmax(final_layer_output)\n",
        "    prediction = np.argmax(prediction_probabilities, axis=1)\n",
        "\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X Shape: (5, 1000)\n",
            "\n",
            "Input shape: (5, 1000)\n",
            "Layer 0 output shape: (5, 256)\n",
            "Layer 1 output shape: (5, 256)\n",
            "Layer 2 output shape: (5, 64)\n",
            "Layer 3 output shape: (5, 64)\n",
            "Layer 4 output shape: (5, 8)\n",
            "Batch Predictions Shape: (5,)\n",
            "Batch Predictions:\n",
            " [1 2 5 4 1]\n"
          ]
        }
      ],
      "source": [
        "# Some Testing\n",
        "batch_size = 5\n",
        "X_features = 1000\n",
        "num_classes = 8  # Possible indices: 0,1,2,3,4,5,6,7\n",
        "\n",
        "X = np.random.randn(batch_size, X_features)\n",
        "print(\"X Shape:\", X.shape)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Input shape:\", X.shape)\n",
        "network_outputs = forward(network, X)\n",
        "for i, out in enumerate(network_outputs):\n",
        "    print(\"Layer {} output shape: {}\".format(i, out.shape))\n",
        "\n",
        "batch_predictions = predict(network, X)\n",
        "print(\"Batch Predictions Shape:\", batch_predictions.shape)\n",
        "print(\"Batch Predictions:\\n\", batch_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdwEd2Gwjw8G"
      },
      "source": [
        "### 1.5. Train and Backprop (2 pts)\n",
        "\n",
        "We implement the training function here. The function takes a network, network input X, and ground truth y as the inputs.\n",
        "When calling this function, we want to have a forward pass and a backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "id": "Yd92p8kMjw8H"
      },
      "outputs": [],
      "source": [
        "def train(network, X, y):\n",
        "    \"\"\"\n",
        "    Train your network on a given batch of X and y only once.\n",
        "    Here are the steps to train once:\n",
        "    1. Run forward to get all layer outputs.\n",
        "    2. Estimate loss and loss_grad.\n",
        "    3. Run layer.backward going from last layer to first.\n",
        "\n",
        "    Note that after you called backward for all layers, the layers with trainable parameters should have already updated.\n",
        "    \"\"\"\n",
        "\n",
        "    # Run forward to get outputs of all layers.\n",
        "    layer_outputs = forward(network, X)\n",
        "\n",
        "    # Since we need the layer input for backward pass, here we get a list of layer input.\n",
        "    # layer_input[i] is an input for network[i].\n",
        "    layer_inputs = []\n",
        "    layer_inputs.append(X)\n",
        "    for curr_out_index in range(len(layer_outputs) - 1):\n",
        "        curr_input = layer_outputs[curr_out_index]\n",
        "        layer_inputs.append(curr_input)\n",
        "\n",
        "    logits = layer_outputs[-1]\n",
        "\n",
        "    # Compute the loss and the initial gradient\n",
        "    loss = softmax_crossentropy_with_logits(logits, y)\n",
        "    loss_grad = grad_softmax_crossentropy_with_logits(logits, y)\n",
        "\n",
        "    # Propagate gradients through network layers using .backward\n",
        "    # Start from last layer and move to earlier layers\n",
        "    for i in range(len(network) - 1, -1, -1):\n",
        "        loss_grad = network[i].backward(layer_inputs[i], loss_grad)\n",
        "\n",
        "    return np.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwAmZNidjw8H"
      },
      "source": [
        "## 2. Test your network (2 pt)\n",
        "\n",
        "In this homework, we use a single-cell dataset: Pancreas Dataset. Our task is to classify cells to cell types based on the gene expression information. There are 8 different cell types. Cells are samples, genes are features, and cell types are classes.\n",
        "\n",
        "You need to load the data from '.csv' and implement a simple dataloader. (If you do not remember why we need a dataloader, you can check Exercise 1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_categorical(cell_labels):\n",
        "    \"\"\"Convert string labels to categorical labels.\n",
        "\n",
        "    Args:\n",
        "        cell_labels: _description_\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "\n",
        "    unique_categories = np.unique(cell_labels)\n",
        "    num_classses = unique_categories.shape[0]\n",
        "\n",
        "    classes_dict = {}\n",
        "    for i in range(num_classses):\n",
        "        cell_type = unique_categories[i]\n",
        "        classes_dict[cell_type] = i\n",
        "\n",
        "    labels = np.zeros((cell_labels.shape[0],))\n",
        "\n",
        "    # Replace the labels with the corresponding index\n",
        "    for i in range(cell_labels.shape[0]):\n",
        "        curr_label = cell_labels[i]\n",
        "        labels[i] = classes_dict[curr_label]\n",
        "\n",
        "    return labels, classes_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Blue': 0, 'Green': 1, 'Red': 2, 'Yellow': 3}\n",
            "[2. 0. 1. 2. 0. 1. 3.]\n"
          ]
        }
      ],
      "source": [
        "# Some testing\n",
        "string_classes = np.array([\"Red\", \"Blue\", \"Green\", \"Red\", \"Blue\", \"Green\", \"Yellow\"])\n",
        "categorical_labels, classes_dict = to_categorical(string_classes)\n",
        "\n",
        "print(classes_dict)\n",
        "print(categorical_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data Report ---\n",
            "Data (inputs) Shape: (8391, 1000). |Features|=1000 |Samples|=8391\n",
            "cell_type_per_sample (Raw Labels) Shape: (8391,)\n",
            "Classes map: {'Pancreas Acinar': 0, 'Pancreas Alpha': 1, 'Pancreas Beta': 2, 'Pancreas Delta': 3, 'Pancreas Ductal': 4, 'Pancreas Endothelial': 5, 'Pancreas Gamma': 6, 'Pancreas Stellate': 7}\n",
            "Labels Shape: (8391,)\n",
            "Labels: [5 0 0 ... 2 2 2]\n",
            "\n",
            "--- Splitting Report ---\n",
            "X Train: (6712, 1000)\n",
            "y Train: (6712,)\n",
            "X Val: (1679, 1000)\n",
            "y Val: (1679,)\n"
          ]
        }
      ],
      "source": [
        "# Load input data from 'data.csv'. Data in csv has 8391 rows and 1000 columns\n",
        "\n",
        "# 8391 rows mean there are 8391 cells.\n",
        "# 1000 columns mean 1000 genes are measured for each cell.\n",
        "\n",
        "# Load labels from 'label.csv' using numpy. Do not import other packages, e.g., pandas.\n",
        "data = np.loadtxt(\"data.csv\", delimiter=\",\")\n",
        "\n",
        "# There are 8391 strings in 'label.csv', and they are the cell type of the 8391 cells.\n",
        "# The input data and labels are aligned:\n",
        "# For example, the first string in 'label.csv' is the cell type of the first cell (frist row of data) of 'data.csv'\n",
        "# In other words, data[0] and cell_type_per_sample[0] should be paired.\n",
        "cell_type_per_sample = np.loadtxt(\"label.csv\", delimiter=\",\", dtype=str)\n",
        "\n",
        "# Get integer labels from the cell types of data type string\n",
        "raw_labels, classes_dict = to_categorical(cell_type_per_sample)\n",
        "labels = raw_labels.astype(int)\n",
        "\n",
        "# Split train and val data. You can choose the ratio of splitting, and you can decide whether to shuffle the data or not.\n",
        "shuffle = True\n",
        "train_ratio = 0.8  # 80% of the data is used for training, and 20% of the data is used for validation\n",
        "X = data\n",
        "y = labels\n",
        "\n",
        "if shuffle:\n",
        "    # Shuffle the data (optional but recommended)\n",
        "    indices = np.arange(len(labels))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    shuffled_labels = labels[indices]\n",
        "    shuffled_inputs = data[indices]\n",
        "\n",
        "    y = shuffled_labels\n",
        "    X = shuffled_inputs\n",
        "\n",
        "num_samples = len(labels)\n",
        "num_train_samples = int(train_ratio * num_samples)\n",
        "num_val_samples = num_samples - num_train_samples\n",
        "\n",
        "X_train = X[:num_train_samples]\n",
        "y_train = y[:num_train_samples]\n",
        "\n",
        "X_val = shuffled_inputs[num_train_samples:]\n",
        "y_val = y[num_train_samples:]\n",
        "\n",
        "print(\"\\n--- Data Report ---\")\n",
        "print(\n",
        "    f\"Data (inputs) Shape: {data.shape}. |Features|={data.shape[1]} |Samples|={data.shape[0]}\"\n",
        ")\n",
        "print(\"cell_type_per_sample (Raw Labels) Shape:\", cell_type_per_sample.shape)\n",
        "print(\"Classes map:\", classes_dict)\n",
        "print(\"Labels Shape:\", labels.shape)\n",
        "print(\"Labels:\", labels)\n",
        "\n",
        "print(\"\\n--- Splitting Report ---\")\n",
        "print(\"X Train:\", X_train.shape)\n",
        "print(\"y Train:\", y_train.shape)\n",
        "\n",
        "print(\"X Val:\", X_val.shape)\n",
        "print(\"y Val:\", y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we have the training and test data. We need a dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": true,
        "id": "iHuBzg4Djw8H"
      },
      "outputs": [],
      "source": [
        "# A mini-dataloader\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "# Note: function signatures should remain the same, but the skeleton is just a guide and can be changed.\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "\n",
        "    indices = np.arange(len(targets))\n",
        "    if shuffle:\n",
        "        # re-order the indices for shuffling the samples\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
        "        indexs = indices[start_idx : start_idx + batchsize]\n",
        "\n",
        "        yield inputs[indexs], targets[indexs]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test without shuffling:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 41665.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0\n",
            "Batch X Shape: (10, 3)\n",
            "Batch y Shape: (10,)\n",
            "---\n",
            "Iteration: 1\n",
            "Batch X Shape: (10, 3)\n",
            "Batch y Shape: (10,)\n",
            "---\n",
            "Iteration: 2\n",
            "Batch X Shape: (10, 3)\n",
            "Batch y Shape: (10,)\n",
            "---\n",
            "\n",
            "\n",
            "Test with shuffling:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 56679.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0\n",
            "Batch X Shape: (10, 3)\n",
            "Batch y Shape: (10,)\n",
            "---\n",
            "Iteration: 1\n",
            "Batch X Shape: (10, 3)\n",
            "Batch y Shape: (10,)\n",
            "---\n",
            "Iteration: 2\n",
            "Batch X Shape: (10, 3)\n",
            "Batch y Shape: (10,)\n",
            "---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Some testing\n",
        "\n",
        "batchsize = 10\n",
        "num_samples = 30\n",
        "X_features = 3\n",
        "\n",
        "inputs = np.random.randn(num_samples, X_features)\n",
        "targets = np.random.randn(\n",
        "    num_samples,\n",
        ")\n",
        "\n",
        "print(\"Test without shuffling:\")\n",
        "for index, batch_data in enumerate(\n",
        "    iterate_minibatches(inputs, targets, batchsize, False)\n",
        "):\n",
        "    batch_X = batch_data[0]\n",
        "    batch_y = batch_data[1]\n",
        "\n",
        "    print(\"Iteration:\", index)\n",
        "    print(\"Batch X Shape:\", batch_X.shape)\n",
        "    print(\"Batch y Shape:\", batch_y.shape)\n",
        "\n",
        "    print(\"---\")\n",
        "\n",
        "print(\"\\n\\nTest with shuffling:\")\n",
        "for index, batch_data in enumerate(\n",
        "    iterate_minibatches(inputs, targets, batchsize, True)\n",
        "):\n",
        "    batch_X = batch_data[0]\n",
        "    batch_y = batch_data[1]\n",
        "\n",
        "    print(\"Iteration:\", index)\n",
        "    print(\"Batch X Shape:\", batch_X.shape)\n",
        "    print(\"Batch y Shape:\", batch_y.shape)\n",
        "\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, test your model! You should get a more than 90% accuracy for training data and a more than 85% accuracy for validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "collapsed": true,
        "id": "3FOu8r1Qjw8H",
        "outputId": "1f76da29-96ca-4518-9499-b6a324cf898c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9\n",
            "Train accuracy: 0.9287842669845053\n",
            "Val accuracy: 0.8999404407385349\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "train_log = []\n",
        "val_log = []\n",
        "\n",
        "for epoch in range(10):\n",
        "    for x_batch, y_batch in iterate_minibatches(\n",
        "        X_train, y_train, batchsize=256, shuffle=True\n",
        "    ):\n",
        "        train(network, x_batch, y_batch)\n",
        "\n",
        "    train_log.append(np.mean(predict(network, X_train) == y_train))\n",
        "    val_log.append(np.mean(predict(network, X_val) == y_val))\n",
        "\n",
        "    clear_output()\n",
        "    print(\"Epoch\", epoch)\n",
        "    print(\"Train accuracy:\", train_log[-1])\n",
        "    print(\"Val accuracy:\", val_log[-1])\n",
        "\n",
        "plt.plot(train_log, label=\"train accuracy\")\n",
        "plt.plot(val_log, label=\"val accuracy\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "264px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
